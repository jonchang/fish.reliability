---
title: Crowdsourced geometric morphometrics enable rapid large-scale collection and
  analysis of phenotypic data
author:
- affiliation: ucla
  email: jonathan.chang\@ucla.edu
  footnote: Author for correspondence
  name: Jonathan Chang
- affiliation: ucla
  name: Michael E. Alfaro
bibliography:
- ../../Mendeley/library.bib
- knitcitations.bib
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: xelatex
    template: inc/mstemplate.tex
    toc: yes
    toc_depth: 1
  html_document:
    toc: yes
  word_document: default
documentclass: inc/msarticle
fontsize: 11pt
geometry: margin=1.2in
layout: preprint
lineno: yes
linestretch: 1.05
csl: inc/evolution.csl
address:
- address: |
    Department of Ecology and Evolutionary Biology, University of California, Los Angeles, CA, USA
  code: ucla
---

```{r prelude, echo=FALSE, message=FALSE, warning=FALSE}
library(MASS)
library(data.table)
library(dplyr)
library(ggplot2)
library(stringr)
library(magrittr)
library(tidyr)
library(reshape2)
library(scales)
library(grid)
library(geomorph)
library(rjson)
library(ipred)
library(directlabels)
library(BAMMtools)
library(knitcitations)
library(kfigr)
options("citation_format"="pandoc")
cleanbib()
```


# Abstract

1. Advances in genomics and informatics have enabled the production of large phylogenetic trees. However, the ability to collect large phenotypic datasets has not kept pace. Here, we present a method to quickly gather geometric morphometric data using crowdsourced image-based landmarking.

2. We show that crowdsourced workers perform equally well as experienced morphologists on the same tasks. We also demonstrate the speed of our method on 7 families of Actinopterygii fishes.

3. Our methods are publicly available as open source software.

# Introduction

The integration of phenotypic data, such as anatomy, behavior, physiology, and other traits, with phylogenies has been a powerful tool for investigating the patterns of biological evolution. Recent advances in next-generation sequencing [@Shendure2008; @Meyer2008] and sequence capture technologies [@Faircloth2012b; @Lemmon2012] have made phylogenetic inference of large radiations of organisms possible [@McCormack2012; @McCormack2013; @Faircloth2013; @Faircloth2014].
These large phylogenies have been successfully used in conjunction with existing comprehensive phenotypic datasets to answer key questions about the mode and tempo of diversifiaction and morphological evolution [@Rabosky2013].

However, similar breakthroughs for generating new phenotypic datasets have been comparatively uncommon, likely due to the high expense and effort required [reviewed in @Burleigh2013]. Creating these large phenotypic datasets has generally required an extended dedicated effort of measuring and describing morphological or behavioral traits that are then coded into a comprehensive data matrix. One such example is the Phenoscaping project [<http://kb.phenoscape.org>; @Deans2015], and its complementary efforts in the Vertebrate Taxonomy Ontogeny [@Midford2013] and Hymenoptera Anatomy Ontology [@Yoder2010], which requires large amounts of researcher effort to compile. Other approaches include using machine learning [@Mckinney2013a] or natural language processing [@Cui2012a] to identify or infer phenotypes. These statistical techniques function ideally with either a large training dataset or a complex model [@Brill2003; @Halevy2009; @Hastie2009], both of which also require intensive researcher effort to build and validate. Finally, methods such as high-throughput infrared imaging, mass spectrometry, and chromatography have been successfully used in plant physiology [@Furbank2011]
 and microbiology [@Skelly2013a], but these methods may not be applicable for zoological researchers.
These bottlenecks in the collection of phenotypic data has limited the taxonomic breadth and depth of research in comparative biology.

Here we present a method and toolkit to efficiently collect two-dimensional geometric morphometric phenotypic data at a "phylogenomic" scale. 
We developed a novel web browser-based image landmarking application, and use Amazon Mechanical Turk (<https://www.mturk.com>) to distribute digitization tasks to remote workers (hereafter *turkers*) over the Internet, who are paid for their contributions.
We also evaluate the accuracy and precision of turkers by assigning identical image sets and digitization protocols to users who are experienced with fish morphology (hereafter *experts*), and compare the inter- and intra-observer differences between turkers and experts.
Finally we demonstrate the utility of this new method by constructing a phylogenetic analysis pipeline, and use this pipeline to download fish photographs from the web, collect Mechanical Turk shape results, and analyze the rate of body shape evolution using BAMM [@Rabosky2014a].

# Methods

## Amazon Mechanical Turk

Amazon Mechanical Turk ("MTurk") is a web-based service where Requesters can request work, known as Human Intelligence Tasks ("HITs") to be performed by Workers. Workers work from home and submit the tasks over the Internet, where Requesters review it, and, if they are satisfied with the results, accept the work and pay the Worker. We use MTurk as a platform to distribute our geometric morphometric tasks and financially compensate the worker accordingly. Scientific collection of data over MTurk and similar services has generally been limited to the fields of psychology and computer science, and there have been few attempts to crowdsource biological trait data [but see @Burleigh2013].

## Web-based geometric morphometrics

![A screenshot of the web app that turkers used to digitize images. A live demonstration is available at <https://jonchang.github.io/eol-mturk-landmark/>](interface.png)

We developed a cross-browser web application that runs completely on the client. This simplifies the infrastructure challenge of needing to serve many crowdsdourced workers simultaneously. The web app is configured with a simple JavaScript Object Notation (JSON) file that describes the landmarks necessary to complete an image digitization task. Below is an example JSON file that demonstrates the utility of our web app:

```json
{
    "C": {
        "kind": "point",
        "help": "Click the center of the eye."
    },
    "D": {
        "kind": "line",
        "help": "Click and drag from the left edge of the eye
                 to the right edge of the eye."
    },
    "O": {
        "kind": "curve",
        "help": "Click and drag over the outline of the eye,
                 starting from the leftmost point of the eye."
    }
}
```

Each digitization task to be completed is given a short abbreviation to aid task identification ("C" for center, "D" for diameter, and "O" for outline), and the type of task, "point", "line" or "curve", for homologous landmarks, linear measurements, and sliding semilandmarks can be specified. There is also an optional short help snippet that is automatically displayed to inform users what to do. These inline help texts complement a larger and more detailed protocol document that workers are required to read before beginning work (Supplementary Material). 

## Server-controlled experiment

Although digitizing and landmarking a single image [microtasks *sensu* @Good2013] is most effective for high-throughput work on MTurk, it is not suited towards conducting controlled experiments. To solve this issue we created a server-side front end that mimiced an official Amazon Mechanical Turk endpoint, but would also automatically distribute tasks in the order required by the experiment. The MTurk facsimile checkpoints each completed image and distributes another from a Requester-defined set of images. A configuration file allows the experimenter to vary the number of replicates required for the given experiment. There is also functionality for non-MTurk workers to contribute data using the same mechanism. The software is open source and uses PHP and SQLite, and can be installed on nearly all web hosting services (<https://github.com/jonchang/fake-mechanical-turk>). 

## Reliability analysis

Collecting landmark-based geometric morphometric data at scale permits detailed analysis of different sources of error, such as among- and within-observer variation [@VonCramon-Taubadel2007]. To assess whether the quality of data gathered by workers recruited through Amazon Mechanical Turk were significantly better or worse than traditionally-collected data, we asked both turkers and experts to landmark a set of 5 fish images, 5 times each. All participants used the same protocol and same software to digitize the same set of fishes.

The landmarks were chosen according to previously-published literature concerning fish shape [@Chakrabarty2005; @Frederich2008], and included a range of landmarks that would be relatively easy to digitize (e.g., the anterior margin of the eye) and ones that would be challenging (the opercular series). A discussion of the performance of turkers versus morphlogists on a per-landmark basis is included in the supplemental material.

We used multiple measurements per worker per image to quantify measurement error within and among observers [@Palmer1986] using *geomorph* `r citep(citation("geomorph"))`. To determine whether turkers and experts were statistically distinguishable, we used the machine learning technique linear discriminant analysis [LDA, @Ripley1996] as implemented in *MASS* `r citep(citation("MASS"))`. A common error in assessing the efficacy of machine learning methods is training the model and then testing the model using the same data set for both. The researcher can make the model increasingly sophisticated to better fit the training dataset, but be unable to make accurate inferences to real-world data, due to overfitting. Conversely, withholding too much data for validation results in underfitting and the inability of the model to generalize its predictions to real data. 

To assess the ability of LDA to distinguish between these two groups, we used $K$-fold cross-validation (CV), which splits our data into $K$ equally-sized groups, using $K-1$ for training and $1$ for validation, and testing the model for all possible permutations of these groups. The special case of $K=N$ groups is also known as leave-one-out CV. We used a value of $K=10$ to validate the LDA model. The statistical literature typically recommends $K=5$ or $K=10$ for its balanced approach to the bias-variance tradeoff [@Kohavi1995; @Hastie2009]. All analyses were performed in R `r citep(citation())`. As an alternate model we also used quadratic discriminant analysis (QDA).

We calculated the per-digitizer median shape for each image used, as well as the consensus turker and morphologist landmarks, and projected these shapes into Procrustes space. We calculated the 95% confidence ellipse to estimate the uncertainty around these shape estimates.

## Phylogenetic analysis on 7 families

To demonstrate the utility and how quickly we can obtain comparative data using this method, we downloaded previously-published phylogenies for 7 fish families (Acanthurdiae, Apogonidae, Balistidae, Chaetodontidae, Labridae, Pomacentridae, and Tetraodontidae) from TreeBASE and Dryad (XXX: CITE TREES) and match them to left-lateral images from the Encyclopedia of Life (<http://eol.org/>) using the Reol interface to the EOL API `r citep(citation("Reol"))`. If there were multiple images for a given species we manually selected the best one. Crowdsourced workers collected body shape data following a standard protocol (Supplementary Material). We then determined the major components of shape variation using a principal components analysis (PCA) as implemented in the R package *geomorph* `r citep(citation("geomorph"))`. 

We used Bayesian Analysis of Macroevolutionary Mixtures [*BAMM*, @Rabosky2014a] to estimate the diversification rate as well as the rate of morphological evolution, using the first six principal components for each of the 7 families. BAMM estimates the location of rate shifts in either diversification or character evolution using a transdimensional (reversible jump) Markov Chain Monte Carlo method, where the number of rate regimes on a phylogeny is allowed to vary. We assessed convergence and mixing using CODA `r citep(citation("coda"))`, BAMMtools `r citep(citation("BAMMtools"))` and Tracer [@rambaut2007tracer]. We then used BAMMtools to identify groups in each family that experienced increased rates of morphological evolution.
	
# Results

## Reliability analysis

```{r cv_table, echo=F, results="asis", anchor="table"}
ip.lda <- function(object, newdata) predict(object, newdata=newdata)$class

no_st <- fread("anon.csv") %>% tbl_df() %>% filter(role != "student") %>% mutate(role=factor(role), mark=factor(mark))
subsamp <- group_by(no_st, fn, role) %>% sample_n(size=400) %>% ungroup()

CV <- subsamp %>% group_by(fn) %>% do(estimator = errorest(role ~ mark:x + mark:y, data=., model=lda, estimator="cv", est.para = control.errorest(k=10), predict=ip.lda))

CV_sum <- CV %>% summarise(fn=fn, error=estimator[["error"]]) %>% separate("fn", "fn", sep="_", extra="drop")

# format for display
CV_sum %>% transmute(`Family`=fn, `Misprediction rate`=error) %>% knitr::kable(digits=3, caption="Misprediction rate of linear discriminant analysis with 10-fold cross validation for each fish image.")
```

The linear discriminant analysis (LDA) with 10-fold cross validation (Table `r figr("cv_table")`) was unable to reliably distinguish between morphologists and crowdsourced workers. The quadratic estimator (QDA) performed similarly poorly (results not shown).

```{r turk_morph_procrustes_space, echo=FALSE, fig.cap="Hello", fig.keep="last", anchor="figure", message=FALSE}

# Load data from the anonymized results file, remove students, drop bad marks
df <- fread("anon.csv") %>% tbl_df() %>%
	filter(role != "student") %>% 
	separate(col=mark, into=c("cat", "cat_seq"), sep=1, remove=F, convert=T) %>%
	mutate(round=floor(sequence/5), fn=factor(str_extract(fn, ".{8}"))) %>%
	filter(!(fn %in% c("Balistid") & mark %in% c("D1","D2")), !mark %in% c("O1","O2","O3","O4","O5", "A3", "D3", "D4"))

# grabs the consensus sequence by person and fish
df <- df %>% group_by(fn, mark, role, sha1mac) %>% summarise(x=median(x), y=median(y))


format_for_gpagen <- function(df) {
	splut <- gather(df, variable, val, x:y) %>% unite(col=id, fn, role, sha1mac)
	xtabs(val ~ mark + variable + id, data=splut)
}


coords <- gpagen(format_for_gpagen(df), ShowPlot=F)$coords


real_plot_tangent_space <- function(coords, axis1="PC1", axis2="PC2") {
  x <- two.d.array(coords)
  pc.res <- prcomp(x)
  pcdata <- pc.res$x
  
	res <- as.data.frame(pcdata)
	res$id <- rownames(res)
	res2 <- res %>% separate(id, into=c("fn", "role","sha"), sep="_") %>% tbl_df()

	# consensus in Procrustes space for downstream highlighting
	dots <- list(
		x=lazyeval::interp(~median(axis1, na.rm=T), axis1=as.name(axis1)),
		y=lazyeval::interp(~median(axis2, na.rm=T), axis2=as.name(axis2)))
	consensus <- res2 %>% group_by(fn, role) %>% summarise_(.dots=dots)

	p <- ggplot(res2, aes_string(axis1, axis2, group="fn")) + geom_point(aes(col=fn, shape=role)) + geom_point(aes(x, y), data=consensus, alpha=0.5, size=5, col="black") + geom_point(aes(x, y, col=fn, shape=role), data=consensus) + stat_ellipse(aes(color=fn), linetype=2, type="norm", level=0.95) + theme_minimal() + theme(legend.position="none")

	direct.label(p, method="smart.grid")
}

real_plot_tangent_space(coords, "PC1", "PC2")
```

Projecting consensus shapes into Procrustes space (Figure `r figr("turk_morph_procrustes_space")`) shows that within-group variation exceeds that of among group variation. The consensus turker shape and consensus expert shape are nearly identical for 5 of 7 families on PC1 and PC6 (Figure `r figr("turk_morph_procrustes_space")`, S1 and S2). 

## Phylogenetic analysis on 7 families

The full data analysis pipeline end-to-end took 2.2 hours of wall-clock time. The BAMMtools analysis identified areas of each phylogeny that were experiencing faster rates of body shape evolution. XXX: Talk about these in more detail.

```{r bamm_plots, echo=F, message=F, warning=F, anchor="figure", cache=T}
results <- fread("fishturk.results") %>% tbl_df() %>% filter(Answer.marks != "") %>% mutate(tip=str_match(annotation, ".*/(.*)/([A-Za-z]+_[A-Za-z]+)")[, 3], family=str_match(annotation, ".*/(.*)/([A-Za-z]+_[A-Za-z]+)")[, 2], Answer.marks=str_replace_all(Answer.marks, '""', '"'))

get_marks <- function(blob, ...) {
  fromJSON(blob) %>% as.data.frame %>% t %>% as.data.frame %>% set_colnames(c("x", "y")) %>% mutate(mark=rownames(.)) %>% cbind(...)
}

allmarks <- rowwise(results) %>% do(get_marks(.$Answer.marks, worker=.$workerid, family=.$family, tip=.$tip))

df <- allmarks %>% group_by(tip, mark, family) %>% summarise(x=median(x), y=median(y))

splut <- gather(df, variable, val, x:y) %>% unite(id, family, tip, sep="|")

res <- xtabs(val ~ mark + variable + id, data=splut)

groups <- str_match(dimnames(res)$id, "(.*)\\|")[, 2] %>% factor %>% as.numeric


full_coords <- gpagen(res, ShowPlot=F)$coords

res <- plotTangentSpace(coords, groups=groups, axis1=1, axis2=2, verbose=T, warpgrids=F)$pc.scores

res <- as.data.frame(res)
res$id <- rownames(res)
res2<-res %>% separate(id, into=c("family", "name"), sep="\\|", extra="drop") %>% tbl_df()

p <- ggplot(res2, aes(x=PC1, y=PC2, col=family)) + geom_point() + theme(legend.position="none")

res2 %>% select(name, PC1) %>% write.table(file="lol.txt", quote=F, row.names=F, col.names=F, sep="\t")
```



# Discussion

Previous work in generating large phenotypic datasets typically relied on one or more of expertise, effort, and computational power.
Collecting phenotypic data remains a large bottleneck in our ability to comprehensively test hypotheses about large radiations of organisms. We developed a method to gather phenotypic data using Amazon Mechanical Turk.
Our results show that the quality of turkers' data are not significantly different from results collected from experts, and that these data can be gathered extremely quickly.

We attempted to distinguish between turker-generated landmarks and expert-generated landmarks using linear discriminant analysis. Even the image with the lowest misprediction rate, from the family `r filter(CV_sum, min(error) == error)$fn`, still incorrectly predicted the digitizer's identity `r filter(CV_sum, min(error) == error)$error %>% as.numeric() %>% scales::percent()` of the time, suggesting that, given a set of landmarks, it is difficult to reliably predict whether a turker or a morphologist created them.

Our novel pipeline to download images, upload them to Amazon MTurk, and process them using BAMM and BAMMtools showcases the ability to rapidly collect phenotypic data. The majority of the 2.2 hours of wall-clock time to collect these data were spent on waiting for worker results; however, a majority of the data had already been collected at the 1-hour mark. An online methodology could conceivably improve on this analysis time, by iteratively refining its results as new data streamed in from Amazon's servers. (BAMM does not support adding new data mid-analysis). Other speedups could be found, particularly in identifying the best image for each species to use for analysis. This could easily be replaced with a protocol for image selection and uploaded to MTurk for categorization.

The results of this pipeline also serve to test previous predictions of the rates of cladogenic speciation and mophological evolution [@Rabosky2013]. Body shape and body size are usually correlated (XXX: find the Cite), and the first PC axis of body shape analyses is typically associated with features related to size. (XXX: make an extensive comparision between the results of the nat comm paper and the 7 families here)

The traditional way of collecting phenotypic data involves enormous researcher effort and significant morphological expertise. For example, @Brusatte2014 used a 853 character discrete character matrix for 150 taxa to estimate the rate of morphological evolution in the transition from theropod dinosaurs to modern birds. These data were collected over the course of 20 years as part of the Therapod Working Group [@Norell2001; @Clark2002; @Hwang2002; @Hwang2004; @Makovicky2005; @Norell2006; @Turner2007; @Turner2012; @dryad_84t75]. @OLeary2013 combined the work of MorphoBank contributors [@OLeary2011] with literature review to generate 4,541 characters for 86 species. @Rabosky2013 examined 7,822 species of ray-finned fish but used a single quantitative measure (body size) to evaluate correlated speciation and morphological evolution.

Newer approaches, typically involving statistical techniques such as machine vision or natural language processing, still require significant researcher expertise in either gathering large training datasets, or to develop new and more sophisticated machine learning models [@Brill2003; @Halevy2009].

Our approach hits a "sweet spot" on the three axes of expertise, effort, and computation. We use researcher expertise to identify a comparative hypothesis, and design a data collection protocol to specifically test this hypothesis. Users on Amazon Mechanical Turk supply a large source of worker effort that collects data per our protocol. Finally, computational statistical techniques validate the accuracy of our data and identify outliers and other errors in data collection. Researchers do not have to spend time digitizing collections, workers need not generate biological hypotheses, and biologists will not have to solve open questions in the fields of machine vision and natural language processing in order to answer questions in comparative biology. The task of phenomic-scale data collection is split up and efficiently allocated according to the strengths of each role, without overly relying on any one axis to carry out the entire task.

We stress that our work does not attempt to, and will likely never displace traditional morphological research and data collection. The role of these traditional techinques is critical for generating preliminary hypotheses. Our method instead performs high-throughput collection of phenotypic data to match the current speeds of molecular data collection. Collecting this data is not useful without having in hand specific hypothesis to test. Our work fills the niche of gathering phenotypic data across large radiations, which is still a challenging open research question. Even seemingly obvious phenotypes, such as the woodiness of plant species, are incomplete and sampled in a biased manner, potentially misleading inference on a global scale [@Fitzjohn2014]. We suggest that our method can play a crucial role in increasing the amount of data that can be brought to bear against comparative hypotheses.

# Acknowledgements

We thank XXX, YYY, and ZZZ for helpful comments on the manuscript, as well as Tina Marcroft, Bruno Frederich, Vanson Liu, Rosalia Agular, Ryan Ellingson, Francesco Pickens, Colette LaRochelle, and the hundreds of Amazon Mechanical Turk workers for contributing their time and efforts. We also thank Dan Rabosky, Brian Sidlauskas, Matthew McGee, Adam Summers, and Michael Burns for useful discussions about fish morphology, and Katie Staab, Tonya Kane, and their respective undergraduate students for serving as guinea pigs during initial evaluation for this study. This work was supported by an Encyclopedia of Life David M. Rubenstein Fellowship (EOL-33066-13) and partially supported by a Stephen and Ruth Wainwright Fellowship to JC. (XXX: Mike's grants?). Travel support to present this research was also provided by the Society for Study of Evolution to JC. 

## Author contributions

Conceived and designed the experiments: JC MEA. Performed the experiments: JC. Analyzed the data: JC. Contributed reagents/materials/analysis tools: JC MEA. Wrote the paper: JC MEA.


## Supplementary material

```{r turker_morph_procrustes_other_pc34, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="PC3 and PC4 for turker / morphologist Procrustes space"}
real_plot_tangent_space(coords, "PC3", "PC4")
```

```{r turker_morph_procrustes_other_pc56, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="PC5 and PC6 for turker / morphologist Procrustes space"}
real_plot_tangent_space(coords, "PC5", "PC6")
```

- Timing figures for turkers
- Detailed comparison between turkers and morphologists

# References

```{r knitcitations_write, echo=FALSE, message=FALSE}
write.bibtex(file="knitcitations.bib")
```

